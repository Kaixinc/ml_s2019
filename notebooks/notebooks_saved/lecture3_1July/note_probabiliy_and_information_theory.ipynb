{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Theory\n",
    "Entropy $\\mathbb{H}(X)$ os a random variable is the measure of uncertainity.\n",
    "\n",
    "- e.g for a discrete random varaible with $S$ state\n",
    "  + $\\mathbb{H}(X) = -\\sum_{s=1}^{s= S} p{(X= s)} \\log {(X = k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## For Binary random variable $X \\in \\{0, 1\\}$\n",
    "Let $p(X=1) = \\theta$, hence $p(X= 0) = 1-\\theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KL divergence or relative entropy\n",
    "to measure the dissimilarity of two probability mass functions(), $p$ and $q$ with $S$ states\n",
    "\n",
    "$\\mathbb{KL}(p|q) = \\sum_{s= 1}^{s= S}  p_s log(\\frac{p_s}{q_s})$ where $p_s = P(X = s)$\n",
    "\n",
    "average number of bits needed to encode data coming from a source with distribution p when we use model q for coodbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross entopy between two discrete probaility distibution\n",
    "$\\mathbb{H}(p,q) = -\\sum_{s}^{S}p_s \\log (q_s) = \\mathbb{H}(p) + \\mathbb{KL}(p, q)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual infomation between two random variable X and Y\n",
    "$\\mathbb{I}(X,Y) = \\mathbb{KL}(\\frac{p(X, Y)}{p(X)p(Y)})$\n",
    "\n",
    "In the homework you will prove relationship between Mutual information, entopy and conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometime we interested in relationship between two random variable in a random experiment\n",
    "\n",
    "- e.g Education and salary\n",
    "- e.g Year and college and credit taken\n",
    "- Image and it's class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $X$ and $Y$ are two random variable then probablity mass function(discrete case) or probability density function\n",
    "defines their joint behavior\n",
    "Like in case of Discrete random variable it statisfiles\n",
    "- $p_{X, Y}(x,y) \\ge 0$\n",
    "- $\\sum_x \\sum_y p(X= x, Y =y) =1$\n",
    "- $p(x, y) = P(X=x, Y=y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| X        | Y           | P(X,Y)  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Cloudy      | Running |  .2  \n",
    "| Cloudy      | Indoor Gym      |  .6  |\n",
    "| Sunny| Running      |   .3  |\n",
    "|Sunny | Indoor Gym|.1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**When we know joint distribution, we can answer any question about event of interest.**\n",
    "\n",
    "- What is the probabity of Running  .2 + .3 = .5 (sum the respective row meeting the condition)\n",
    " + This is called marginalization of pobability. i.e marginal probabiliy mass function is $p_X(x)= \\sum_y p_{X,Y (x,y)$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- What is probability of Running given that it is Cloudy(conditional probability) $P(Y= Running|X = Cloudy)$ = $\\frac{P(Y= Running, X= Cluody)}{P(X = Cloudy)} = $ \n",
    "\n",
    "$  \\frac {P(Y= Running, X= Cloudy)}{ \\sum_{Y = {Running, Indoor Game} \\;P(X = Cloudy, Y)}}$ = $\\frac {P(Y= Running, X= Cloudy)}{P(Y= Running, X= Cloudy) + P(Y= Indoor Game, X= Cloudy)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- conditional probability can be written in joint form\n",
    "- Marginal probability can be written in joint form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that in supervised setting we want to learn\n",
    "$f:X\\rightarrow Y \\sim P(Y|X)$  but due to random nature of world(?)\n",
    "\n",
    "we are not commiting to a fixed value of Y=y for a given value of X=x, instead we want to learn whole distribution of values $Y$ for given value of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we can easily learn/record the dataset of possible values of X and Y and build $P(X, Y)$\n",
    "\n",
    "are we done ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In reality\n",
    "## Data is high dimensional\n",
    "## Impossible to have enough observations\n",
    "## We start putting more structure on the data though probabilty distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's talk again about Bayesian view for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# probability density function of beta distribution is \n",
    "<center>\n",
    "<font size = '6'>\n",
    "\n",
    "$f(\\theta; a, b) = \\frac{\\Gamma{(a +b)}}{\\Gamma{a} \\Gamma{b}} \\theta^{a-1} \\theta^{b-1}$ for $0 \\le \\theta \\le1$ and $a,b \\ge 0$\n",
    "</font>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact,IntSlider\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f(a,b):\n",
    "    theta = np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), 400)\n",
    "    plt.plot(theta, beta.pdf(theta, a, b),\n",
    "    'r-', lw=5, alpha=0.6, label='beta pdf')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "    print(a,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15de619f4574f48a480df1176277557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='a', max=400), IntSlider(value=10, description='b'), Outâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(a, b)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(f, a= widgets.IntSlider(min=0,max=400,step=1,value=10), b =widgets.IntSlider(min=0,max=100,step=1,value=10))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
