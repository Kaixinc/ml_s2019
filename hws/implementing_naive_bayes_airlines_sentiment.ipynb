{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size =6, color=\"marquee\"> Twitter US Airline Sentiment Classification using \n",
    "    navie bayes generative classifier </font></center>\n",
    "\n",
    "  https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "  \n",
    "  Data can be downloaded from the following link. You have to login to the kaggle site to get access to the data.\n",
    "Unzip the data in folder where notebook is running. There should be a file called **Tweet.csv**\n",
    "\n",
    "https://www.kaggle.com/crowdflower/twitter-airline-sentiment/downloads/twitter-airline-sentiment.zip/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1529990131237-cfa5ce9517b6?ixlib=rb-1.2.1&auto=format&fit=crop&w=1949&q=80\"  width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise will help us to get started with navie bayes generative classifier and using tools(pandas, sklearn etc) from python eco system. Most of the code is already written but please understand it.\n",
    "\n",
    "**Problem statement:** classify sentiment based on text as positive or negative* using **naive bayes** in supervised machine setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** See the link above\n",
    "\n",
    "\n",
    "credit:\n",
    "\n",
    "\n",
    "- some of the images are from  https://unsplash.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd # for data analysis\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to install following libraries too. Please uncomment to install\n",
    "#! pip install scikit-learn\n",
    "#! pip install gensim\n",
    "#!pip install -U setuptools\n",
    "#!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'Tweets.csv'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id,airline_sentiment,airline_sentiment_confidence,negativereason,negativereason_confidence,airline,airline_sentiment_gold,name,negativereason_gold,retweet_count,text,tweet_coord,tweet_created,tweet_location,user_timezone\r\n",
      "570306133677760513,neutral,1.0,,,Virgin America,,cairdin,,0,@VirginAmerica What @dhepburn said.,,2015-02-24 11:35:52 -0800,,Eastern Time (US & Canada)\r\n",
      "570301130888122368,positive,0.3486,,0.0,Virgin America,,jnardino,,0,@VirginAmerica plus you've added commercials to the experience... tacky.,,2015-02-24 11:15:59 -0800,,Pacific Time (US & Canada)\r\n",
      "570301083672813571,neutral,0.6837,,,Virgin America,,yvonnalynn,,0,@VirginAmerica I didn't today... Must mean I need to take another trip!,,2015-02-24 11:15:48 -0800,Lets Play,Central Time (US & Canada)\r\n",
      "570301031407624196,negative,1.0,Bad Flight,0.7033,Virgin America,,jnardino,,0,\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\",,2015-02-24 11:15:36 -0800,,Pacific Time (US & Canada)\r\n",
      "570300817074462722,negative,1.0,Can't Tell,1.0,Virgin America,,jnardino,,0,@VirginAmerica and it's a really big bad thing about it,,2015-02-24 11:14:45 -0800,,Pacific Time (US & Canada)\r\n",
      "570300767074181121,negative,1.0,Can't Tell,0.6842,Virgin America,,jnardino,,0,\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\r\n",
      "it's really the only bad thing about flying VA\",,2015-02-24 11:14:33 -0800,,Pacific Time (US & Canada)\r\n",
      "570300616901320704,positive,0.6745,,0.0,Virgin America,,cjmcginnis,,0,\"@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\",,2015-02-24 11:13:57 -0800,San Francisco CA,Pacific Time (US & Canada)\r\n",
      "570300248553349120,neutral,0.634,,,Virgin America,,pilot,,0,\"@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\",,2015-02-24 11:12:29 -0800,Los Angeles,Pacific Time (US & Canada)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10  Tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it is a comma separated csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First step before jumping into using any machine learning model is understanding the data by **describing it's statistical attribute and visualizating samples or sample property**.\n",
    "We can use CSV file reader and try to accomplish above task. But as they say python is a language with **battery(libraries) included**. Let's use **pandas and matplotlib** libraries to do this task as cleanly as possible. What to describe and what to plot will be an essential skill we build as we do various data science or machine learning tasks. Also with time you will also built a knowledge of various packages available for different domain in python eco system. Most of the time reading blog and google search does the job of finding right libraries. Various packages for download and installation are avaiable at [PyPI - the Python Package Index](https://pypi.python.org/pypi)\n",
    "\n",
    "**Optional**\n",
    "This is [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "If you have more time look into this link [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.dEdNuDM)\n",
    "\n",
    "Let's load the csv file in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570300767074181121</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:33 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "5  570300767074181121          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "5     Can't Tell                     0.6842  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "5                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
       "5  2015-02-24 11:14:33 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see how wrapping the file in pandas simplify lot of tasks\n",
    "airlines_sentiment_df = pd.read_csv('Tweets.csv')\n",
    "airlines_sentiment_df.head(6) # there are other functions like tail and sample to check record in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's select observation with positive and negative sentiment only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_sentiment_df = airlines_sentiment_df[airlines_sentiment_df.airline_sentiment.isin(['positive', 'negative'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>570227911803998208</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bsauce2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir there is no local agent! There is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 06:25:03 -0800</td>\n",
       "      <td>Fort Myers</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>570241830757007360</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ChrisNielsen2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united I received 1 bag last night, I am stil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 07:20:21 -0800</td>\n",
       "      <td>Hoops City (Memphis, TN)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3765</th>\n",
       "      <td>568143658089717761</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>herestorian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united offered me a voucher to switch flights...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 12:22:58 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>570251909447192577</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yorkshire2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@USAirways @Beamske how about a real live pers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 08:00:24 -0800</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>568515101319254016</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>christinerimay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united Got a call. Bag is to be delivered ton...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-19 12:58:57 -0800</td>\n",
       "      <td>Someplace saving something</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7825</th>\n",
       "      <td>569227490046164992</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.6822</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.3629</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BernardLeCroix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue yes, last month when Boston got the f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 12:09:44 -0800</td>\n",
       "      <td>Providence, RI</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>568702964623216643</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dan_roam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you United! “@united: @dan_roam That's a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-20 01:25:27 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>568228257037856768</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sjking2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united I'm asking if you can simplify the com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 17:59:08 -0800</td>\n",
       "      <td>California, US</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>569847788986462209</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>0.6684</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JoshSeefried</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir wasn't just a delay. Your counter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 05:14:34 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5064</th>\n",
       "      <td>569397984519016448</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Cancelled Flight</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lnghurdoncurRIE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir  Cancelled Flighted Sunday 9:50A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 23:27:13 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "12331  570227911803998208          negative                        1.0000   \n",
       "626    570241830757007360          negative                        1.0000   \n",
       "3765   568143658089717761          negative                        1.0000   \n",
       "9070   570251909447192577          negative                        1.0000   \n",
       "3338   568515101319254016          positive                        1.0000   \n",
       "7825   569227490046164992          negative                        0.6822   \n",
       "3088   568702964623216643          positive                        1.0000   \n",
       "3624   568228257037856768          negative                        0.6304   \n",
       "13496  569847788986462209          negative                        1.0000   \n",
       "5064   569397984519016448          negative                        1.0000   \n",
       "\n",
       "               negativereason  negativereason_confidence     airline  \\\n",
       "12331  Customer Service Issue                     1.0000    American   \n",
       "626              Lost Luggage                     1.0000      United   \n",
       "3765   Customer Service Issue                     0.3520      United   \n",
       "9070             Lost Luggage                     1.0000  US Airways   \n",
       "3338                      NaN                        NaN      United   \n",
       "7825               Can't Tell                     0.3629       Delta   \n",
       "3088                      NaN                        NaN      United   \n",
       "3624   Customer Service Issue                     0.6304      United   \n",
       "13496             Late Flight                     0.6684    American   \n",
       "5064         Cancelled Flight                     1.0000   Southwest   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "12331                    NaN       bsauce2011                 NaN   \n",
       "626                      NaN    ChrisNielsen2                 NaN   \n",
       "3765                     NaN      herestorian                 NaN   \n",
       "9070                     NaN    yorkshire2002                 NaN   \n",
       "3338                     NaN   christinerimay                 NaN   \n",
       "7825                     NaN   BernardLeCroix                 NaN   \n",
       "3088                     NaN         dan_roam                 NaN   \n",
       "3624                     NaN       sjking2000                 NaN   \n",
       "13496                    NaN     JoshSeefried                 NaN   \n",
       "5064                     NaN  lnghurdoncurRIE                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "12331              0  @AmericanAir there is no local agent! There is...   \n",
       "626                0  @united I received 1 bag last night, I am stil...   \n",
       "3765               0  @united offered me a voucher to switch flights...   \n",
       "9070               1  @USAirways @Beamske how about a real live pers...   \n",
       "3338               0  @united Got a call. Bag is to be delivered ton...   \n",
       "7825               0  @JetBlue yes, last month when Boston got the f...   \n",
       "3088               0  Thank you United! “@united: @dan_roam That's a...   \n",
       "3624               0  @united I'm asking if you can simplify the com...   \n",
       "13496              0  @AmericanAir wasn't just a delay. Your counter...   \n",
       "5064               0  @SouthwestAir  Cancelled Flighted Sunday 9:50A...   \n",
       "\n",
       "      tweet_coord              tweet_created              tweet_location  \\\n",
       "12331         NaN  2015-02-24 06:25:03 -0800                  Fort Myers   \n",
       "626           NaN  2015-02-24 07:20:21 -0800    Hoops City (Memphis, TN)   \n",
       "3765          NaN  2015-02-18 12:22:58 -0800                         NaN   \n",
       "9070          NaN  2015-02-24 08:00:24 -0800                    Kentucky   \n",
       "3338          NaN  2015-02-19 12:58:57 -0800  Someplace saving something   \n",
       "7825          NaN  2015-02-21 12:09:44 -0800              Providence, RI   \n",
       "3088          NaN  2015-02-20 01:25:27 -0800                         NaN   \n",
       "3624          NaN  2015-02-18 17:59:08 -0800              California, US   \n",
       "13496         NaN  2015-02-23 05:14:34 -0800                         NaN   \n",
       "5064          NaN  2015-02-21 23:27:13 -0800                         NaN   \n",
       "\n",
       "                    user_timezone  \n",
       "12331  Eastern Time (US & Canada)  \n",
       "626                           NaN  \n",
       "3765                          NaN  \n",
       "9070   Eastern Time (US & Canada)  \n",
       "3338   Eastern Time (US & Canada)  \n",
       "7825   Eastern Time (US & Canada)  \n",
       "3088                          NaN  \n",
       "3624   Pacific Time (US & Canada)  \n",
       "13496  Central Time (US & Canada)  \n",
       "5064   Central Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's view some random record\n",
    "airlines_sentiment_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_sentiment_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only airline_sentiment and text filed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_sentiment_df = airlines_sentiment_df[['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our  data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$ $x_i$ is text field and $y_i$ is airlines_sentiment(positive or negative)**. Using using this we will train(learn parameters $\\theta$ of a models(Naive bayes, Discriminant anlaysis based etc.)) and use trained model to classify new text as positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    object\n",
       "text                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_sentiment_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "airline_sentiment attribute is not in proper format but we will use it's string value later to create class label(positive=1, negative =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    0\n",
       "text                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_sentiment_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset doesn't contain any nan and  looks clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we should do an extensive EDA(exploratory data analysis) on this data to see distribution of various attribute on **positive** and **negative** class. This can give some more insight into distriubtion of text/words in these two classes and help in further cleanup of text  data.\n",
    "\n",
    "We'll go ahead use the current text field and sentiment label to build naive bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer only understand scalar or vector or matrices. We need to convert text to vectors(feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our  message.\n",
    "\n",
    "### Bog of words model for document:\n",
    "\n",
    "In BOW  we treat document as collection of word without any order. \n",
    "\n",
    "- **Bernoulli document model:** message is represented by a binary feature vector of *absence or presence of word*.\n",
    "- **Multinomial document model**: message is represented by an *integer feature vector of word frequency*.\n",
    "\n",
    " Later we will see\n",
    "there are better model for sentence or document representation **where words order matters**. There are model which takes into account the word order like [N-gram](https://en.wikipedia.org/wiki/N-gram) etc.\n",
    "Infact, Deep learning has enabled us to learn even better embedding of words using context of words(co-occurrence ).\n",
    "We will try to use them in **deep learning section** [**optional** see [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's build Multinomial document model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to vector is bit involved and require a good understanding of NLP(natural language processing).\n",
    "\n",
    "But in general, to convert a message into vector we need to:\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words(lemmatization, stemming), removing stop words, Capital form(Cow vs cow) handling etc.\n",
    "3. Build a dictionary of words and map the messages into vector of counts using this dictionary.\n",
    "4. Finally train a  Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again can use a python library [nltk](https://www.nltk.org/) to perform above task as per our need(in tools1 we did similar exercises) but current exercise we'll use another common library, **gensim** to do the basic cleanup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's only do [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' see how it works on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'machine/NN', b'learn/VB']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"Hi Machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see apart from lemmatizing we also get part of speech attached to the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(.75 points). Finish the following lambda so that we only capture lemmatized words. *apply* will apply  written lambda to each text value.\n",
    "\n",
    "Hint:\n",
    "Your preprocessed_messages(a pandas Series) should look like\n",
    "\n",
    "1    [virginamerica, ve, add, commercial, experienc...\n",
    "\n",
    "3    [virginamerica, really, aggressive, blast, obn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finish this lambda\n",
    "preprocessed_messages= airlines_sentiment_df.text.apply(lambda text: ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how lemmatization looks likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [virginamerica, ve, add, commercial, experienc...\n",
       "3    [virginamerica, really, aggressive, blast, obn...\n",
       "4             [virginamerica, really, big, bad, thing]\n",
       "5    [virginamerica, seriously, pay, flight, seat, ...\n",
       "6    [virginamerica, yes, nearly, time, fly, vx, ea...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is very basic clean and further clean can be done like removing more stop words, spell correction etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first text, see how **added** to changed to **add**. We can do some more pre processing  with some justifucation but let's go\n",
    "ahead with current manipulation.\n",
    "\n",
    "There are lots of other natural language processing libraries in python for performing above activities like\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [nltk](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the fundamental concern in machine learning is *Generalization*, **how well our machine is going to work on unseen/future data. Does it generalize well on future data**?\n",
    "- If we wanted to do well on given data, why  would we even bother to build an algorithm. We can just store the data and do a lookup for any message.\n",
    "\n",
    "We will come back to this question later in the course when we talk about **model selection and evaluation**.\n",
    "\n",
    "One simple way to answer to above question is to hide some portion of dataset and use remaining dataset for building the model i.e. learning the parameters of the model. Once we have build the model, we can report some number/measure on hidden dataset to tell how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_labels = airlines_sentiment_df.airline_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's partition our data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X (9232,) and train Y (9232,)\n",
      "Shape of test X (2309,) and test Y (2309,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_messages, test_messages, training_labels, test_labels = train_test_split(preprocessed_messages,\n",
    "                                                                                 messages_labels,\n",
    "                                                                                test_size= .2,\n",
    "                                                                                 stratify= messages_labels)\n",
    "print('Shape of training X {} and train Y {}'.format(training_messages.shape, training_labels.shape))\n",
    "print('Shape of test X {} and test Y {}'.format(test_messages.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use training messages only for building the model**\n",
    "\n",
    "- We need to convert each message into count vector. Where a positive/negative message is mapped to vector representing each word frequency in the message\n",
    "    + To do this we need to build a dictionary of words first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2(.25 point) Write code to update the python set. We are using set as it takes care of duplicate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word = set()\n",
    "\n",
    "for message in training_messages:\n",
    "    # write your code here to update unique_word set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color=\"red\"> See, how dictionary is build using only training data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8208"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in vocabulary\n",
    "num_unique_words = len(unique_word)\n",
    "num_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert each **num_messages** messages into **num_unique_words** dimensional vector. Each entry in the vector counts number of times a word occurs in the text.  In machine earning we call activity like this **feature engineering**. \n",
    "\n",
    "First we need to assign each word a unique location/index in a **num_unique_words** dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let' first  build a default dictionary to assign each word a unique location in the feature vector\n",
    "from collections import defaultdict, Counter\n",
    "word_to_index_dict = defaultdict(int)\n",
    "for index , word in enumerate(unique_word):\n",
    "    word_to_index_dict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a reverse dictionary  for mapping index to word. It will help in debugging etc.\n",
    "# See how we used dictionary comprehension\n",
    "index_to_word_dict = { value:key  for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9232,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_messages = training_messages.shape\n",
    "num_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a numpy integer matrix of **num_messages X num_unique_words**, initialized with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9232, 8208)\n"
     ]
    }
   ],
   "source": [
    "# each row in training_X is our x_i\n",
    "training_X = np.zeros((len(training_messages), len(unique_word)), dtype=int)\n",
    "print(training_X.shape)\n",
    "test_X = np.zeros((len(test_messages), len(unique_word)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(text, word_to_index_dict):\n",
    "    feature = np.zeros((len(word_to_index_dict),), dtype=int)\n",
    "    word_freq =  Counter(text)\n",
    "    # setting the word count in text_no row of text_features\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in word_to_index_dict:\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            feature[index_of_word] = freq\n",
    "    return feature        \n",
    "for text_no, text in enumerate(training_messages):\n",
    "    training_X[text_no] = build_feature(text, word_to_index_dict)\n",
    "    \n",
    "for text_no, text in enumerate(test_messages):\n",
    "    test_X[text_no] = build_feature(text, word_to_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How we check if it above code is right ?\n",
    "## Let's do some primitive checking on a text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'be': 2, 'flight': 1, 'today': 1, 'other': 1, 'appear': 1, 'weather': 1, 'inclement': 1, 'dca': 1, 'southwestair': 1, 'plane': 1, 'left': 1, 'come': 1})\n",
      "##Encoding for text no 7 in feature vector is ##\n",
      "southwestair 1\n",
      "appear 1\n",
      "left 1\n",
      "be 2\n",
      "other 1\n",
      "inclement 1\n",
      "come 1\n",
      "today 1\n",
      "dca 1\n",
      "plane 1\n",
      "flight 1\n",
      "weather 1\n"
     ]
    }
   ],
   "source": [
    "text_no =7\n",
    "message_word_count = Counter(training_messages.iloc[text_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in text_features to see if count is set properly\n",
    "print('##Encoding for text no {} in feature vector is ##'.format(text_no))\n",
    "for i, count in enumerate(training_X[text_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red' > Make sure Counter and encoding gives same results in above cell </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have successfully converted messages into a numpy matrix of feature vectors.\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1532081274113-50aa6c277925?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1950&q=80\" alt=\"Well done\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert positive and negative label to 1 and 0  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4666    positive\n",
       "6868    negative\n",
       "3905    negative\n",
       "5614    negative\n",
       "228     negative\n",
       "9239    negative\n",
       "7913    positive\n",
       "Name: airline_sentiment, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.tail(7)# can check from head too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our training_y value\n",
    "training_y = (training_labels.values == 'positive').astype(int)\n",
    "test_y = (test_labels.values == 'positive').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our text samples. \n",
    "\n",
    "Let review some theory and see what parameters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a text $x_i$  to a class c= POSITIVE or c= NEGATIVE which has maximum value of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modeling class conditional densities, we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from text document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{ij}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values and there are total $D$ different unique values(word). This definetly suits a **$D$ side die** situation. \n",
    "\n",
    "**Infact, once we have learned $P(x_{ij}|c)?$ i.e probabilites of different sides for positive and negative die,**\n",
    "** positive or negative text generation in bag of word model is nothing but rolling positive or negative die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "<font size = 6> \n",
    "$P(x_i|c) = \\frac{n_i !}{\\prod_j^D x_{ij|C}!} P(c) \\prod^{D} P(w_j|c)^{x_{ij}} \\propto P(c) \\prod^{D} P(w_j|c)^{x_{ij}}$ \n",
    "</font>\n",
    "as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "\n",
    "- Hence the parameters are nothing  nothing but relative frequency of $w_j$ in documents of class c=NEGATIVE or c= POSITIVE\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "- We can sum our numpy text_feature matrix along row or dim 0 to get total frequency of each feature for positive and negative class\n",
    "- normalize total frequency of each feature with total frequency of all the features for each class.\n",
    "- prior class  densities are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are number of document/text in class k.\n",
    "\n",
    "- So we have to learn **num_unique_words** parameters for each die\n",
    "- and 2  class densities.\n",
    "\n",
    "\n",
    "# Let's learn the parameters for c= positive(1) and c= negative(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1890, 8208)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.        , 0.00011417,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First estimate for positive\n",
    "\n",
    "# summing up per feature count\n",
    "training_X_positive = training_X[training_y ==1]\n",
    "print(training_X_positive.shape)\n",
    "per_feature_count =np.sum(training_X_positive, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_positive = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's estimate parameters for negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3(.5 Point): Write code similarly to negative class to estimate following parameter parameters_w_negative for negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7342, 8208)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.12010932e-05, 1.19851698e-03, 1.12010932e-05, ...,\n",
       "       1.12010932e-05, 7.84076526e-05, 1.12010932e-05])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write code here\n",
    "\n",
    "\n",
    "print(parameters_w_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero probability issue\n",
    "As we can see some of the probability can be zero. It will create problem when we estimate probability of a new document in test set if that was not in training set. \n",
    "\n",
    "If any of the term in product is zero it will result in zero product. If any of the class don't have this term then probability of this document for any class will be zero. If we play log trick for comparing product of probability, we will be in trouble as log of zero is not defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One way to handle this situtation to add a fake 1 count of the word in each class. This is called Laplace law of succession or add one smoothing**.\n",
    "\n",
    "We estimate\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c) + 1}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c) + |V|}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function and $|V|$ is size of our dictionary.\n",
    "\n",
    "This can be done by adding a row of ones to training_X_positive and training_X_negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once we learn have learned the model(i.e its parameters, probabilities of different words occuring in positive die and negative die), how good is our model ?\n",
    "- Let take out our test data convert to count feature vector using same dictionaries\n",
    "- Calculate the probability if test data belonging to Positive or negative. i.e if probability is >=.5 Positive otherwise negative\n",
    " or we can calculate the ratio\n",
    " <font size = 5>\n",
    " $\\frac{P(x_{test}|c=positive)}{P(x_{test}|c=negative)} = \\frac{ P(c=positive)  \\prod^{D}_{j =1} P(w_j|c=positive)^{x_{test,j}}} { P(c= negative)\\prod^{D}_{j=1} P(w_j|c=negative)^{x_{test, j}}}$ \n",
    " </font>\n",
    " \n",
    " **Note:Generally such large product of probabilities, turns out to be zero because of computer representation limits of real numbers.**\n",
    " \n",
    " Another option is let take log on right hand side and after some manipulation one can show that if\n",
    " <font size = 5>\n",
    "  $\\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=positive)) +log(P(c= positive)) \\ge log(P(c=negative))+ \\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=negative))$\n",
    "  \n",
    "  </font>\n",
    "  \n",
    " then it is positive otherwise negative\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "- Compare with test data label and let's report accuracy and confusion matrix\n",
    "\n",
    "Let's do these steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a class for our multinomial naive bayes and estimate its new parameters based on  Laplace law of succession or add one smoothing.\n",
    "\n",
    "You will use this same class on test challenge datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4 (3 = .5 + 2(1+1)+ .5) Perform the following activity in the following multinomial_naive_bayes class \n",
    "\n",
    "- Implement \\_calculate\\_prior function\n",
    "- Finish fit function\n",
    "- Finish accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multinomial_naive_bayes(object):\n",
    "    \n",
    "    NEGATIVE_CLASS, POSITIVE_CLASS = range(2)\n",
    "    \n",
    "    def __init__(self, class_prior=(.5, .5), fit_prior=True):\n",
    "        \n",
    "        self._class_prior= class_prior\n",
    "        self._fit_prior = fit_prior\n",
    "        \n",
    "        \n",
    "    def fit(self, training_X, training_y):\n",
    "        '''\n",
    "        Finishi this function. Store the parameters\n",
    "        for positive class in self._parameters_w_positive\n",
    "        for negative class in self._parameters_w_negative\n",
    "        See how predict method relies on these paramters.\n",
    "        '''\n",
    "        #Write code here for estimating self._parameters_w_positive and self._parameters_w_negative\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self._fit_prior:\n",
    "            self._class_prior = self._calculate_prior(len(training_X_positive), len(training_X_negative))\n",
    "         \n",
    "    def _calculate_prior(self, n_positive, n_negative):    \n",
    "        '''\n",
    "        Finish this function to return prior of negative and positive class\n",
    "        comment or remove the NotImplementedError \n",
    "        '''\n",
    "        raise NotImplementedError(\"implement this!\")\n",
    "        \n",
    "        \n",
    "    def _calculate_score(self, parameters,test_text, class_prior):\n",
    "            return np.sum(np.log(np.power(parameters,test_text))) + class_prior            \n",
    "        \n",
    "    def predict(self, test_X, test_y):\n",
    "        '''\n",
    "        predict the accuracy\n",
    "        '''\n",
    "        positive_score = np.zeros_like(test_y,dtype=float)\n",
    "        negative_score = np.zeros_like(test_y,dtype=float)\n",
    "        positive_score.shape, negative_score.shape# just printing to make sure shape is right\n",
    "\n",
    "\n",
    "        for idx, test_text in enumerate(test_X):# this will fetch row by row, encoded test messages\n",
    "            positive_score[idx] = self._calculate_score(self._parameters_w_positive,\n",
    "                                                        test_text,\n",
    "                                                        np.log(self._class_prior[multinomial_naive_bayes.POSITIVE_CLASS]))\n",
    "            negative_score[idx] = self._calculate_score(self._parameters_w_negative,\n",
    "                                                        test_text, np.log(self._class_prior[multinomial_naive_bayes.NEGATIVE_CLASS]))\n",
    "\n",
    "        ## Write code here\n",
    "        #Look at the equation above  for deciding the class label and finish \n",
    "        # the code for accuracy calculation\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = multinomial_naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(training_X, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7080987440450411"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Following code will show why python eco system shines.\n",
    "We will use python library sklearn to build  multinomial Naive Bayes classifier </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5 (.5 = .25(training part) +.25 point(evaluation part)) Write code here to train MultinomialNB classifer  from sklearn and report accuracy in test set\n",
    "\n",
    "Check if you get same accuracy number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Write the code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
